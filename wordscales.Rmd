---
title: "wordscores"
author: "Uni Lee"
date: "11/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(tidyverse)
library(readtext)
library(quanteda)
theme_set(theme_minimal())
```

# Scaling models with Wordscores 

Wordscores compare the word frequencies of text to references to evaluate similarity between them. It is highly automated and doesn't require prior knowledge about language. 

* Calculate the probability of a word appearing in given texts
* Take the weighted average of the probabilities using word scores from references. 

This is not about sentiment, but more about political position. That is, the wordscores represent the scale of political pereferences (parties, left/right, etc.) 
 
## Estimating policy positions from political texts (Laver&Garry)

To generate party positions for British and Irish manifestos, they hand-coded scheme similar to the CMP's. 

Assumptions of this exercise are:
1) Manifesto content is related to party policy positions
2) Word usage is related to policy positions
3) Word usage is constant over time 
4) All relevant words are covered in the reference texts 

First, obtain wordscores using reference texts (conditional probabilities of getting word x). Then, Apply the wordscores by calculating the weighted mean score of the words in text. Consider variance to estimate uncertainty. 

## Exercise: annual budget speeches held in the Irish Parliament from 2008 - 2012

We will use `quanteda.textmodels` library to do this. 

```{r}
library(quanteda.textmodels)
# Load the data
data(data_corpus_irishbudget2010, package = "quanteda.textmodels")

# Tokenize it and reate a dfm
budget_dfm <- dfm(tokens(data_corpus_irishbudget2010))

```

Then, create reference scores to identify the positions of new documents. Assume we know reference scores only for the 5th and 6th documents. 

```{r}
refscores <- c(rep(NA, 4), 1, -1, rep(NA, 8)) # There are 14 speeches in the dfm, so we create a vector of 17 elements. Only the 4th and 5th elements have reference scores, which are assigned based on previous studies. 
```

Using the reference scores, we can predict positions of the rest of the documents based on the positions of reference texts. We will use `textmodel_wordscores` to calculate this. 

```{r}
ws <- textmodel_wordscores(budget_dfm, y = refscores, smooth = 1)
head(coef(ws))
```

We can use the `textplot_scale1d()` function from the `quanteda.textplots` package to plot word positions.

```{r, eval = FALSE}
library(quanteda.textplots)
textplot_scale1d(ws,
                 highlighted = c("cut", "minister", "welfare", "economy", "budget"), 
                 highlighted_color = "red")
```

## Predict document positions 

Use the predict function as if this were a regular fitted regression model. 

```{r}
ws_pred <- predict(ws, interval = "confidence")
ws_pred
```

Finally, we can plot party positions of each speaker. 

```{r, echo = FALSE}
textplot_scale1d(ws_pred, margin = "documents",
                 groups = docvars(data_corpus_irishbudget2010, "party"))
```

# Wordfish

Wordfish builds a statistical model (Poisson model) that explains the occurrence of each word. 

## Poisson model

- Dependent variables of interest may be counts, e.g.

    + Occurence of conflict/wars, casualties in conflicts; Number of bills brought forward in a term; Number of hospitalizations, sicknesses etc.
    + **Word count in a document**

- A dependent count variable $\gamma$

    + bound between 0 and $\infty$
    + takes only discrete values (0,1,2,3,...)
    
## Poisson model

**Poisson distribution**

$$\gamma_i = Possion(\lambda_i)$$

- Poisson distribution: Repeated Bernoulli-Experiments (0/1) 
- Generally used in count data (poisson regression)
- Has only one parameter: 'Event occurrence rate'
- No contagion effects; the event rate remains constant

$$\gamma_{ij} \sim Poisson(\lambda_{ij})$$
$$\lambda_{ij} = exp(\alpha_i + \psi_j + \beta_j * \omega_i)$$

- $i$ = document (e.g. party manifesto)
- $j$ = unique word

- $\alpha_i$ = document fixed effect
- $\psi_j$ = word fixed effect
- $\beta_j$ = word specific weight (sign representing the ideological direction)
- $\omega_i$ = document position

- Regression without independent variables

    + Solution: Maximum Likelihood Estimation
    
1. Estimate party parameters conditional on the expectation for the
word parameters (in first iteration the starting values)

2. Estimate word parameters conditional on party parameters
obtained in previous step

3. Go back and forth until a convergence criterion is met and the
likelihoods do not change anymore

## Wordfish estimation

**Likelihood function**

$$\sum^m_j \sum^n_i - exp(\alpha_i + \psi_j + \beta_j * \omega_i) + ln(exp(\alpha_{it} + \psi_j + \beta_j * \omega_i)) * \gamma_{ij} $$

Without fixing some parameters, there are infinite combinations of $\omega$
and $\beta$, which could provide the same likelihood.

- Document positions: mean of all positions $\omega$ across all elections is
set to 0, and standard deviation to 1. 

- Set directionality (e.g. document A always has a smaller valuer than document B). 
- Set document fixed effect: first document $\alpha$ is set to 0.


This means that you **cannot directly compare estimates ACROSS different estimations** (for example, in secondary analysis). This is the case for ALL scaling models (e.g. Nominate), and also for Wordscores.

- Think to what extent position estimates are actually comparable...

    + ... across countries
    + ... over time
    + ... between documents

## Wordfish estimation

- Dimension of the scaling is created ex post (as compared to Wordscores)

    + What is the dimension identified?
    + More validation required
    + Creation of alternative dimensions via subsetting texts only

- Not multilangual! Only works in English (could use high-quality translation, such as EU Parliament speeches)

## Exersise: US Senate debate on partial birth abortion

```{r}
library(quanteda)
load("data/corpus_us_debate_speaker.rda")
summary(corpus_us_debate_speaker)
```

Tokenize and create dfm. 

```{r}
stops <- c(setdiff(stopwords(),  
                   c("her", "she", "hers", "he", "his", "him")),
           "bill", "can") # Create a list of stop words 

toks <- tokens(corpus_us_debate_speaker,  
               remove_punct = TRUE, 
               remove_symbols = TRUE,  
               remove_numbers = TRUE) %>%
  tokens_remove(stops) %>%
  tokens_tolower() # Tokenize and clean tokens
 
senate_dfm <- dfm(toks) # dfm it
senate_dfm
```

Run the wordfish model (one-dimensional scaling model)

```{r}
library(quanteda.textmodels)
wf <- textmodel_wordfish(senate_dfm, dir = c(3, 21)) # We use the `dir` argument to set the polar opposites of the scale. In this case we assume that that the 3rd speaker Barbara Boxer, is to the left of the 21st speaker Rick Santorum.

summary(wf)
```

Estimate speaker positions using the predict function. 

```{r}
preds <- predict(wf, interval="confidence")
preds
```

We can extract the estimated speaker positions into a data frame and merge in the docvars.

```{r}
speaker_pos <- data.frame(preds$fit, 
                          speaker = row.names(preds$fit)) %>% 
  dplyr::left_join(docvars(senate_dfm)) %>% 
  dplyr::arrange(fit)
head(speaker_pos)
```

Plot speaker positions and word positions/frequencies. 

```{r}
library(quanteda.textplots)
textplot_scale1d(wf, groups = corpus_us_debate_speaker$party)
textplot_scale1d(wf, margin = "features",  alpha = 0.2,
                 highlighted = c("life", "choice", "womb", "her", "woman", "health",
                                 "born", "baby", "gruesome", "kill", "roe", "wade",
                                 "medical", "her", "his", "child", "religion","doctor"),
                 highlighted_color = "red")

## Make it look nicer using ggplot2
library(ggplot2)
textplot_scale1d(wf, margin = "features",  alpha = 0.2,
                 highlighted = c("life", "choice", "womb", "her", "woman", "health",
                                 "born", "baby", "gruesome", "kill", "roe", "wade",
                                 "medical", "her", "his", "child", "religion","doctor"),
                 highlighted_color = "red") + 
  labs(x = "Word position", y = "Word frequency") +
  ggtitle("Estimated word positions for use senate debate on abortion") + 
  theme_minimal()

```
## Homework

```{r}
coal <- data_corpus_EPcoaldebate # EU coal subsidies debate in 2010
summary(coal)

coal_tok <- tokens(coal,  
               remove_punct = TRUE, 
               remove_symbols = TRUE,  
               remove_numbers = TRUE) %>%
  tokens_tolower() # Tokenize and clean tokens

coal_tok_stop <- tokens_remove(coal_tok, stopwords("en")) # remove stopwords

coal_dfm <- dfm(coal_tok_stop) # create a dfm
coal_dfm

# Fit a wordfish model to the dfm
wf <- textmodel_wordfish(coal_dfm, dir = c(3, 21)) # We use the `dir` argument to set the polar opposites of the scale. In this case we assume that that the 3rd speaker Barbara Boxer, is to the left of the 21st speaker Rick Santorum.

summary(wf)

```

