---
title: "Assignment 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(readtext)
library(quanteda)
library(dplyr)
library(tidyr)
library(seededlda)
library(stm)
library(kableExtra)
theme_set(theme_minimal())
```


## Load and Prepare the data set

1. Load the `corpus_us_debate_speaker` from the data folder on the course's github repository main site, reshape the corpus into paragraphs and select only those paragraphs with at least 8 words in them. Create a tokens object in which words are converted to lower case and remove numbers, punctuation, common stop words, and tokens with less than two characters. Convert the tokens object to a dfm and trim it to include only tokens that appear at least in 7.5% of the documents and at most in 90% of the documents.

```{r}
# Load corpus
load("data/corpus_us_debate_speaker.rda") 
summary(corpus_us_debate_speaker, n=5)

# Reshape the corpus into paragraphs and subset 
speeches_para <- corpus_reshape(corpus_us_debate_speaker, to="paragraphs")
speeches_para <- corpus_subset(speeches_para, ntoken(speeches_para) >= 8)

table(ntoken(speeches_para))

# Create tokens object
para_tokens <- tokens(speeches_para,
                      remove_punct = TRUE,
                      remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords()) %>%
  tokens_select(min_nchar=2)

# Convert to dfm 
para_dfm <- dfm(para_tokens) 

para_dfm
```


2. Use the `convert()` function from `quanteda` to convert the dfm to an stm.

```{r, warning=FALSE}
para_stm <- convert(para_dfm,
        to="stm")
```

## Identify optimal number of topics

3. Fit five different structural topic models with different numbers of topics for $K$  = 10, 20, 30, 40, 50 respectively. Note that this might take a while to run and converge.

```{r, message=FALSE}

model1 <- stm(para_stm$documents,
              para_stm$vocab,
              verbose=FALSE,
              K=10)

model2 <- stm(para_stm$documents,
              para_stm$vocab,
              verbose=FALSE,
              K=20)

model3 <- stm(para_stm$documents,
              para_stm$vocab,
              verbose=FALSE,
              K=30)

model4 <- stm(para_stm$documents,
              para_stm$vocab,
              verbose=FALSE,
              K=40)

model5 <- stm(para_stm$documents,
              para_stm$vocab,
              verbose=FALSE,
              K=50)


```


4. Create a diagnostic plot showing the held-out likelihood, the residuals, the semantic coherence of the topics, and the lower bound. Also create a plot to contrast the semantic coherence with the exclusivity of the topics, i.e. how much each topic has its own vocabulary not used by other topics. Explain your results and decide on an optimal number of topics to continue with.

To choose the number of topics, we can consult several quantitative diagostic values by using `searchK()` function. The Held-out likelihood estimates the probability of words appearing within a document when those words have been removed from the document (conditional probability). In other words, the higher the likelihood, the higher the model's prediction performance. Overdispersion of residuals indicates that more topics are needed to soak up the variance. Lastly, semantic coherence is maximized when the most probable words in a given topic frequently co-occur together. However, this could happen just by having a few topics dominated by very common words. To check the validity of this measure, we must contrast it with exclusivity, as shown in the second graph created using `plotModels()` function. In our example, I will choose K=30. First, I ruled out k=50 because its residuals were too high. k=30 had the highest held-out likelihood than the rest. Note that the results are different for each run, so the numbers I provide here may be different from results of the current run.

```{r}
# Diagnostic Values 
search <- searchK(
  documents = para_stm$documents,
  vocab = para_stm$vocab,
  K = c(10, 20, 30, 40, 50),
  init.type="LDA",
  verbose=FALSE
)

plot(search)

# For k=30, 
# Check semantic coherence vs. exclusivity to check topic quality
k_30 <- selectModel(para_stm$documents, 
            para_stm$vocab, 
            K = 30, 
            data = para_stm$meta, seed = 8458159,
            verbose=FALSE,
            runs = 10
            )
plotModels(k_30,
  xlab = "Semantic Coherence",
  ylab = "Exclusivity",
  labels = 1:length(k_30$runout),
  legend.position="bottomleft")
```


## Create topic labels and explore topic prevalence

5. After you selected your preferred topic model (k=30), explore the word probabilities ($\beta$) and create meaningful labels for each topic. For some topics, identify documents that are very representative for a those particular topics. Discuss what some of your topics are about in a bit more detail.


```{r}
#k=30
labelTopics(model3)
plot.STM(model3, "summary", n=5)

# Create a table of beta (word probabilities)
beta <- tidytext::tidy(model3)
beta

# Create a plot to compare beta across topics
beta_2 <- beta %>% group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(term = reorder_within(term, beta, topic))
beta_2

ggplot(beta_2, aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics")

```


6. Extract the topic proportions ($\theta$) from the model and plot the prevalence of each topic across the overall corpus. Also create a perspective plot visualizing the combination of two topics and discuss the results.

## Fit an stm with covariates

7. Refit your favorite stm model but this time include a covariant for party affiliation into the model (i.e. saying that party affiliation impacts topic prevalence). Once the model converged, estimate the effect that party affiliation has on the prevalence of two topics, i.e. are Democrats or Republicans more likely to speak about either of the topics. Create a plot of your estimated effects and discuss your results.

## Summarize

8. Summarize your findings across all tasks in a paragraph or two.

## Submission form and deadline

- Deadline: 30 November
- Submission form: submit your code, plots, and discussion of the results in a single RMarkdown file.



