---
title: "Topic models"
author: "Uni Lee"
date: "11/17/2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidytext)
library(readtext)
library(quanteda)
library(dplyr)
library(tidyr)
theme_set(theme_minimal())
```

# Topic models 

Topic modeling is a method for unsupervised classification of documents, finding natural groups of items even when we are not sure what we are looking for. They are exploratory probability models that work with large amount of text with a thematic structure. Still may need some input that steers classification. 

## Latent Dirichlet Allocation (LDA)

Estimates the bag of words associated with each topic, and the bag of topics that describe each document. 

1) Every document is a mixture of topics.
2) Every topic is a mixture of words. 

Assuming that some number of topics exists for the whole collection of documents, each document is generated by first choosing a distribution over the topics, then, for each word, choosing a topic assignment and choosing the word from the corresponding topic. Each piece of the structure is a random variable. 

Topic models give: 

    + a probabilistic view of the relationship between W, Z and $\theta$
    + a full statistical framework for learning most aspects of the relationship
    
but take away substantive control. You do not get to assert what the topics mean.

## Topic model: Gibbs sampling

LDA uses Gibbs sampling, an algorithm for successively sampling conditional distributions of variables, estimating weights for each word. 

As output, you get a vector of words for each topic. Based on those, you make decisions on what to focus you analysis on. 

## Evaluation 

Always always validate your results using: 

- Human evaluation (does the hierarchy of topics make sense?)
- Statistical evaluation 
- The model fit, K, topic relationships
- Word precision, topic coherence

## Choosing K: number of topics expected in the output

- Choose fewer K for macro-level topics
- Choose larger K for micro-level topics 

## Variations

- Seeded LDA
- Structural topic models (STM)
- Expressed agenda model 
- Correlated topic models 

## Exercise using seeded LDA

```{r}
library(seededlda)
library(quanteda)

load("data/corpus_us_debate_speaker.rda")
summary(corpus_us_debate_speaker, n=5)

# Speeches are too big. Could be helpful to reshape to paragraphs? But check if the results are valid

speeches_para <- corpus_reshape(corpus_us_debate_speaker, to="paragraphs")
head(summary(speeches_para))
table(ntoken(speeches_para)) # 15 paragraphs without tokens

# Let's subset paragraphs that contain at least 8 words, remove punctuation, numbers, stop words, and tokens with less than 2 characters. 
speeches_para <- corpus_subset(speeches_para, ntoken(speeches_para) > 7)

para_tokens <- tokens(speeches_para,
                      remove_punct = TRUE,
                      remove_numbers = TRUE) %>%
  tokens_remove(stopwords()) %>%
  tokens_select(min_nchar=2)

para_dfm <- dfm(para_tokens)
para_dfm

# Assume there are 10 topics. 

para_lda <- textmodel_lda(para_dfm, k=10)

# Most important terms 
terms(para_lda, 15) %>% kableExtra::kable()
# Take a look at the results and see what each topic is about based on the most occuring words. 

# Plot distribution of words for each topic
terms_df <- as_tibble(para_lda$phi) %>% # in the lda dataframe, the parameter we are interested in is called "phi"
  mutate(topic=1:10) %>%
  gather(term, beta, -topic) %>% # renaming "phi" values to "beta"
  group_by(topic) %>%
  slice_max(beta, n=10) %>%
  ungroup() %>%
  arrange(topic, -beta)

terms_df %>%
  mutate(term=reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend=FALSE) +
  facet_wrap(~topic, scales="free") +
  scale_y_reordered()
```
```{r}
# Assign the most likely topic for each document as a new variable in the docvars. 
para_dfm$topic <- topics(para_lda)

# Use the most important terms as the label for each topic 
top_terms <- terms(para_lda, 4)
topic_names <- apply(top_terms, 2, paste, collapse="_")
topic_names

# Plot prevalence of topics across all documents
topic_names_df <- dplyr::bind_rows(topic_names) %>% 
  gather(topic, names)

topics_df <- as_tibble(para_lda$theta) %>% # Theta shows how often a topic appears across all documents
  mutate(document = rownames(.)) %>% 
  gather(topic, gamma, -document) %>% 
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>% 
  left_join(topic_names_df, by = "topic")

topics_df %>%
  ggplot(aes(reorder(topic, gamma), gamma, label = names, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.2),
                     labels = scales::percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Topic Prevalence",
       subtitle = "With the top words that contribute to each topic")
```


